# Preprocessing Data: 

In this simple Linear Regression model, we use the data set with outliers and understand Outlier Treatment

Outlier Treatment Methods:
1) Capping and Flooring :
* Impute all the values above 3* P99 (99th percentile) and below 0.3* P1 (1 percentile)
* Impute with values 3* P99 and 0.3* P1
* You can use any multiplier instead of 3, as per your business

2) Exponential smoothing 
* Extrapolate curve between P95 and P99 and cap all the values falling outside to the value generated by the curve
* Similarly, extrapolate curve between P5 and P1

3) Sigma Approach 
* Identify outliers by capturing all the values falling outside u+-xsigma
* You can use any multiplier as x, as per your business requirement.



Missing Value Imputation: 
(Data can have missing values for a number of reasons such as observations that were not recorded and data corruption) 
1) Impute with Zero / Remove rows with missing data from your dataset :
* Impute missing values with zero. 
* When we have a large dataset and only few values missing, then we can delete those values and still have substantial number of values to train and test the model. But if the number of missing values is high, then removing observations is not advisable. 

2) Impute values with mean/median values in your dataset.  
* In cases when there are large number of missing values, we need to replace values with some values which should not impact the model much. Such neutral values are values closer to the center of the variable i.e choose mean or median to replace the missing values.
* For numerical variables, impute missing values with Mean or Median
* For categorical variables, impute missing values with Mode. 

3) Segment based imputation
* Identify relevant segments
* Calculate mean/median/mode of segments
* Impute the missing value according to the segments
* For example, we can say rainfall hardly varies for cities in particular state
* In this case, we can impute missing rainfall value of a city with the average of that state



Seasonality :
Seasonality is the presence of variations that occur at specific regular intervals less than a year, such as weekly, monthly, or quarterly.
Reasons: Weather, vacation and holidays

In order to normalize them into the same scale we can follow
* Calculate multiplication factor
 m =  myu (year) / myu (month)
 * Multiply each observation with its multiplication factor
 


Bivariate Analysis:
It is the simultaneous analysis of two variables (attributes). It explores the concept of relationship between two variables, whether there exists an association and the strength of this association, or whether there are differences between two variables and the significance of these differences.

1) Scatterplot:
* Scatter indicates the type(linear or non-linear) and strength of the relationship between two variables

2) Correlation: 
* Linear correlation quantifies the strength of a linear relationship between two numerical variables
* When there is no correlation between two variables, there is no tendency for the values of one quantity to increase or decrease with the values of the second quantity.
* Correlation is used to drop non usable variables.



Correlation:
Correlation is a statistical measure that indicates the extent to which two or more variables fluctuate together. A positive correlation indicates the extent to which those variables increase or decrease in parallel; a negative correlation indicates the extent to which one variable increases as the other decreases. 

* Correlation coefficients is a way to put a value to the relationship
* Correlation coefficients have a value of between -1 and 1.
* '0' means there is no relationship between the variables at all
* While -1 and 1 means that there is a perfect negative or positive correlation

Correlation Matrix:
Is a table showing the correlation coefficients between variables. Each cell in the table shows the correlation between two variables. A correlation matrix is used as a way to summarize data, as an input into a more advanced analysis and as a diagnostic for advanced analysis. 
Application: To identify collinearity in the data. To summarize a large amount of data where the goal is to see patterns. 
Note: Need to avoid multi-collinearity. 

Causation: The relation between something that happens and the thing that causes it. The first thing that happens is the cause and the second thing is the effect. 


